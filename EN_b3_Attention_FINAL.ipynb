{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n!unzip -qq Flickr8k_Dataset.zip\n!unzip -qq Flickr8k_text.zip\n!rm Flickr8k_Dataset.zip Flickr8k_text.zip","metadata":{"id":"9hI-A0-zFN5p","execution":{"iopub.status.busy":"2022-01-08T19:57:09.940333Z","iopub.execute_input":"2022-01-08T19:57:09.940954Z","iopub.status.idle":"2022-01-08T20:01:51.983984Z","shell.execute_reply.started":"2022-01-08T19:57:09.94086Z","shell.execute_reply":"2022-01-08T20:01:51.983041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom skimage import io\n\nimport collections\nimport random\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport re\nfrom glob import glob\nimport pickle\nfrom os import listdir\nfrom collections import Counter\n\nfrom pickle import dump\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Model","metadata":{"id":"mAjuulpp9f0P","execution":{"iopub.status.busy":"2022-01-08T20:01:51.987437Z","iopub.execute_input":"2022-01-08T20:01:51.987739Z","iopub.status.idle":"2022-01-08T20:01:58.348046Z","shell.execute_reply.started":"2022-01-08T20:01:51.987699Z","shell.execute_reply":"2022-01-08T20:01:58.345661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use this if you're working with official EN Repo\n# import efficientnet.tfkeras as efn ","metadata":{"id":"FtCVMqY7_yYA","execution":{"iopub.status.busy":"2022-01-08T20:01:58.349346Z","iopub.execute_input":"2022-01-08T20:01:58.350833Z","iopub.status.idle":"2022-01-08T20:01:58.355087Z","shell.execute_reply.started":"2022-01-08T20:01:58.350789Z","shell.execute_reply":"2022-01-08T20:01:58.354384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this if want to use TPU, skip if you're using GPU","metadata":{"id":"E1ATNIddqkfV","execution":{"iopub.status.busy":"2022-01-08T20:01:58.357797Z","iopub.execute_input":"2022-01-08T20:01:58.35843Z","iopub.status.idle":"2022-01-08T20:01:59.336354Z","shell.execute_reply.started":"2022-01-08T20:01:58.358395Z","shell.execute_reply":"2022-01-08T20:01:59.335431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try:\n#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n# except ValueError:\n#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"id":"NfWoILFbwJHt","execution":{"iopub.status.busy":"2022-01-08T20:01:59.338811Z","iopub.execute_input":"2022-01-08T20:01:59.339153Z","iopub.status.idle":"2022-01-08T20:02:00.280663Z","shell.execute_reply.started":"2022-01-08T20:01:59.339109Z","shell.execute_reply":"2022-01-08T20:02:00.27954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = './Flicker8k_Dataset/'\nannotations = './Flickr8k.token.txt'","metadata":{"id":"lc-6uHov5Jo4","execution":{"iopub.status.busy":"2022-01-08T20:02:00.282417Z","iopub.execute_input":"2022-01-08T20:02:00.282739Z","iopub.status.idle":"2022-01-08T20:02:02.033592Z","shell.execute_reply.started":"2022-01-08T20:02:00.282697Z","shell.execute_reply":"2022-01-08T20:02:02.030396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_filter(image_path):\n  v= list()\n  for i in image_path:\n    s= os.path.basename(i)\n    s = s.split('.')[0]\n    v.append(s)\n  return v","metadata":{"id":"mya75QvOnh0I","execution":{"iopub.status.busy":"2022-01-08T20:02:02.035317Z","iopub.execute_input":"2022-01-08T20:02:02.035624Z","iopub.status.idle":"2022-01-08T20:02:02.349019Z","shell.execute_reply.started":"2022-01-08T20:02:02.035583Z","shell.execute_reply":"2022-01-08T20:02:02.348164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_name_vector = glob(PATH + '*.jpg')\nprint(\"The total images present in the dataset: {}\".format(len(all_img_name_vector)))\nprint(all_img_name_vector[-1])","metadata":{"id":"hawyMn-anzDc","outputId":"10a32d94-a1ca-4389-872d-7d604c4c4bf5","execution":{"iopub.status.busy":"2022-01-08T20:02:02.350242Z","iopub.execute_input":"2022-01-08T20:02:02.351686Z","iopub.status.idle":"2022-01-08T20:02:02.386733Z","shell.execute_reply.started":"2022-01-08T20:02:02.351642Z","shell.execute_reply":"2022-01-08T20:02:02.386003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_name_vector_filtered= image_filter(all_img_name_vector)","metadata":{"id":"obRSVOACxVnE","execution":{"iopub.status.busy":"2022-01-08T20:02:02.38797Z","iopub.execute_input":"2022-01-08T20:02:02.388351Z","iopub.status.idle":"2022-01-08T20:02:02.40197Z","shell.execute_reply.started":"2022-01-08T20:02:02.388316Z","shell.execute_reply":"2022-01-08T20:02:02.401264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_img_name_vector_filtered[-1])\nprint(all_img_name_vector[-1])","metadata":{"id":"AZkt_izPnpZ3","outputId":"7a03f1c6-25d2-43c9-a1fc-8669c051f523","execution":{"iopub.status.busy":"2022-01-08T20:02:02.405482Z","iopub.execute_input":"2022-01-08T20:02:02.405831Z","iopub.status.idle":"2022-01-08T20:02:02.413367Z","shell.execute_reply.started":"2022-01-08T20:02:02.40575Z","shell.execute_reply":"2022-01-08T20:02:02.412687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image(images, captions=None, cmap=None ):\n  \"\"\"\n        Parameters:\n              images (list of str): A list of the path of images\n\n        Returns:\n              None\n  \"\"\"\n  f, axes = plt.subplots(1, len(images), sharey=True)\n  f.set_figwidth(15)\n   \n  for ax,image in zip(axes, images):\n      ax.imshow(io.imread(image), cmap)\n        \nplot_image(all_img_name_vector[8089:])","metadata":{"id":"urP9WOOYIIB9","outputId":"6879dea2-6284-44f2-c333-7c073dda977f","execution":{"iopub.status.busy":"2022-01-08T20:02:02.414631Z","iopub.execute_input":"2022-01-08T20:02:02.41538Z","iopub.status.idle":"2022-01-08T20:02:02.857873Z","shell.execute_reply.started":"2022-01-08T20:02:02.415343Z","shell.execute_reply":"2022-01-08T20:02:02.855916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_img_name_vector)","metadata":{"id":"t4IRQ8n332aO","outputId":"a1141d82-de6d-40dc-8e84-383dfbc25603","execution":{"iopub.status.busy":"2022-01-08T20:02:02.858885Z","iopub.execute_input":"2022-01-08T20:02:02.859112Z","iopub.status.idle":"2022-01-08T20:02:02.865232Z","shell.execute_reply.started":"2022-01-08T20:02:02.859083Z","shell.execute_reply":"2022-01-08T20:02:02.864585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load doc into memory\ndef load_doc(filename):\n  # open the file as read only\n  file = open(filename, 'r')\n  # read all text\n  text = file.read()\n  # close the file\n  file.close()\n  return text\n \n# load descriptions\ndoc = load_doc(annotations)","metadata":{"id":"FQpOGOJ058wa","execution":{"iopub.status.busy":"2022-01-08T20:02:02.866821Z","iopub.execute_input":"2022-01-08T20:02:02.867448Z","iopub.status.idle":"2022-01-08T20:02:02.880023Z","shell.execute_reply.started":"2022-01-08T20:02:02.86741Z","shell.execute_reply":"2022-01-08T20:02:02.879061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract descriptions for images\ndef load_descriptions(doc):\n\tmapping = dict()\n\t# process lines\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\tif len(line) < 2:\n\t\t\tcontinue\n\t\t# take the first token as the image id, the rest as the description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# remove filename from image id\n\t\timage_id = image_id.split('.')[0]\n\t\t# convert description tokens back to string\n\t\timage_desc = ' '.join(image_desc)\n\t\t# create the list if needed\n\t\tif image_id not in mapping:\n\t\t\tmapping[image_id] = list()\n\t\t# store description\n\t\tmapping[image_id].append(image_desc)\n\treturn mapping\n \n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))","metadata":{"id":"VjVF3tmp64Z9","outputId":"df2b1797-6e1c-4c1d-ec1a-fea5f5d9a9f8","execution":{"iopub.status.busy":"2022-01-08T20:02:02.882131Z","iopub.execute_input":"2022-01-08T20:02:02.882656Z","iopub.status.idle":"2022-01-08T20:02:02.987973Z","shell.execute_reply.started":"2022-01-08T20:02:02.882616Z","shell.execute_reply":"2022-01-08T20:02:02.987146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Preprocessing(image_tag, captions):\n  \"\"\"\n        Parameters:\n              image_tag (list of str): A list of tags of filtered images\n              captions (list of str): A list of preprocessed cations\n        Returns:\n              train_captions (list of str): A list of captions ready for training\n              img_name_vector (list of str): A list of image tags correseponing to train_captions\n  \"\"\"\n  train_captions = []\n  img_name_vector = []\n\n  for image_path in image_tag:\n    caption_list = captions[image_path]\n    train_captions.extend(caption_list)\n    img_name_vector.extend([image_path] * len(caption_list))\n\n  return train_captions, img_name_vector","metadata":{"id":"gyTYFrZM66MQ","execution":{"iopub.status.busy":"2022-01-08T20:02:02.989371Z","iopub.execute_input":"2022-01-08T20:02:02.989634Z","iopub.status.idle":"2022-01-08T20:02:02.995072Z","shell.execute_reply.started":"2022-01-08T20:02:02.989599Z","shell.execute_reply":"2022-01-08T20:02:02.994398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_augmentation(train_caption):\n  \"\"\"\n        Parameters:\n              train_captions (list of str): A list of captions ready for training\n        Returns:\n              Augmented captions with <start> and <end> keys ready for training\n  \"\"\"\n  train_captions2= []\n  for aug in train_caption:\n    caption = f\"<start> {aug} <end>\"\n    train_captions2.append(caption)\n\n  return train_captions2","metadata":{"id":"tpJt4hj9P36R","execution":{"iopub.status.busy":"2022-01-08T20:02:02.996792Z","iopub.execute_input":"2022-01-08T20:02:02.997401Z","iopub.status.idle":"2022-01-08T20:02:03.008247Z","shell.execute_reply.started":"2022-01-08T20:02:02.997362Z","shell.execute_reply":"2022-01-08T20:02:03.007528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_captions, img_name_vector = Preprocessing(all_img_name_vector_filtered, descriptions)","metadata":{"id":"wOOIfqnRwOpS","execution":{"iopub.status.busy":"2022-01-08T20:02:03.011063Z","iopub.execute_input":"2022-01-08T20:02:03.011327Z","iopub.status.idle":"2022-01-08T20:02:03.024577Z","shell.execute_reply.started":"2022-01-08T20:02:03.011302Z","shell.execute_reply":"2022-01-08T20:02:03.023862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_captions2= image_augmentation(train_captions)","metadata":{"id":"6aBFPBOb0xGB","execution":{"iopub.status.busy":"2022-01-08T20:02:03.025686Z","iopub.execute_input":"2022-01-08T20:02:03.026374Z","iopub.status.idle":"2022-01-08T20:02:03.046513Z","shell.execute_reply.started":"2022-01-08T20:02:03.026335Z","shell.execute_reply":"2022-01-08T20:02:03.045662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the different between tf example and this is i use the pic number and tf example use the full path\n# i use all_img_name_vector_filtered and tf use all_img_name_vector so keep that in mind\n# so i need to add PATH + img_name_vector[i] + '.jpg'","metadata":{"id":"eFQwsPbKX9Lb","execution":{"iopub.status.busy":"2022-01-08T20:02:03.04804Z","iopub.execute_input":"2022-01-08T20:02:03.048329Z","iopub.status.idle":"2022-01-08T20:02:03.055717Z","shell.execute_reply.started":"2022-01-08T20:02:03.048265Z","shell.execute_reply":"2022-01-08T20:02:03.055031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_captions[0])\nImage.open(PATH + img_name_vector[0] + '.jpg')","metadata":{"id":"ep7MICxlYchG","outputId":"2204f400-9780-4a6d-fcc9-c1f7af82a472","execution":{"iopub.status.busy":"2022-01-08T20:02:03.057162Z","iopub.execute_input":"2022-01-08T20:02:03.057522Z","iopub.status.idle":"2022-01-08T20:02:03.140083Z","shell.execute_reply.started":"2022-01-08T20:02:03.057486Z","shell.execute_reply":"2022-01-08T20:02:03.139376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Debugging\n\nrefined = list()\nfor i in range(len(img_name_vector)):\n  refined.append(PATH + img_name_vector[i] + '.jpg')\n","metadata":{"id":"PJVh09W2d6N0","execution":{"iopub.status.busy":"2022-01-08T20:02:03.141242Z","iopub.execute_input":"2022-01-08T20:02:03.141962Z","iopub.status.idle":"2022-01-08T20:02:03.166409Z","shell.execute_reply.started":"2022-01-08T20:02:03.141922Z","shell.execute_reply":"2022-01-08T20:02:03.165714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"refined[0]","metadata":{"id":"elhAFVTJ4KFD","outputId":"4624f8b5-e69b-4fda-b417-a7fe64484666","execution":{"iopub.status.busy":"2022-01-08T20:02:03.167898Z","iopub.execute_input":"2022-01-08T20:02:03.168157Z","iopub.status.idle":"2022-01-08T20:02:03.173862Z","shell.execute_reply.started":"2022-01-08T20:02:03.168123Z","shell.execute_reply":"2022-01-08T20:02:03.172946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use this when running the offical repo\n\n# target_size = (300, 300,3)\n# def load_image(image_path):\n#     # image_path = PATH + image + '.jpg'\n#     img = tf.io.read_file(image_path)\n#     img = tf.image.decode_jpeg(img, channels=3)\n#     img = tf.image.resize(img, (target_size[0],target_size[1])) \n#     img = efn.preprocess_input(img)\n    \n#     return img, image_path","metadata":{"id":"dAnbkOEBIn81","execution":{"iopub.status.busy":"2022-01-08T20:02:03.175596Z","iopub.execute_input":"2022-01-08T20:02:03.176171Z","iopub.status.idle":"2022-01-08T20:02:03.182769Z","shell.execute_reply.started":"2022-01-08T20:02:03.176133Z","shell.execute_reply":"2022-01-08T20:02:03.181944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use 'with open' when running on TPU instead of tf.io.read_file(image_path)\n\ntarget_size = (300, 300,3)\ndef load_image(image_path):\n    # with open(image_path, \"rb\") as local_file: \n    #   raw = local_file.read()\n    raw = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(raw, channels=3)\n    img = tf.image.resize(img, (target_size[0], target_size[1]))\n    img = tf.keras.applications.efficientnet.preprocess_input(img)\n    return img, image_path\n# load_image('/content/Flicker8k_Dataset/1357689954_72588dfdc4.jpg')","metadata":{"id":"0xvtRw0Tlu5F","execution":{"iopub.status.busy":"2022-01-08T20:02:03.184396Z","iopub.execute_input":"2022-01-08T20:02:03.184767Z","iopub.status.idle":"2022-01-08T20:02:03.195601Z","shell.execute_reply.started":"2022-01-08T20:02:03.18473Z","shell.execute_reply":"2022-01-08T20:02:03.194807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp= '../input/efficientnet-keras-noisystudent-weights-b0b7/noisystudent/noisy.student.notop-b3.h5'\nimage_model= tf.keras.applications.EfficientNetB3(weights=pp, input_shape=target_size, include_top=False)\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"id":"lavEn245rv7C","execution":{"iopub.status.busy":"2022-01-08T20:02:03.19711Z","iopub.execute_input":"2022-01-08T20:02:03.197408Z","iopub.status.idle":"2022-01-08T20:02:09.99025Z","shell.execute_reply.started":"2022-01-08T20:02:03.197331Z","shell.execute_reply":"2022-01-08T20:02:09.989519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use it with EN Repo, note that the weights are alr in it\n\n# image_model = efn.EfficientNetB3(weights='noisy-student', input_shape=target_size, include_top=False)\n\n# new_input = image_model.input\n# hidden_layer = image_model.layers[-1].output\n\n# image_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"id":"3rwoTNV3bURh","execution":{"iopub.status.busy":"2022-01-08T20:02:09.991507Z","iopub.execute_input":"2022-01-08T20:02:09.991761Z","iopub.status.idle":"2022-01-08T20:02:09.99713Z","shell.execute_reply.started":"2022-01-08T20:02:09.991728Z","shell.execute_reply":"2022-01-08T20:02:09.996377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tqdm","metadata":{"id":"mI5A-WyDoz6G","outputId":"968c9c85-f666-415f-ce87-8f637c430d1c","execution":{"iopub.status.busy":"2022-01-08T20:02:09.998591Z","iopub.execute_input":"2022-01-08T20:02:09.999189Z","iopub.status.idle":"2022-01-08T20:02:18.672233Z","shell.execute_reply.started":"2022-01-08T20:02:09.999149Z","shell.execute_reply":"2022-01-08T20:02:18.671425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"id":"yJzf60_ao0A8","execution":{"iopub.status.busy":"2022-01-08T20:02:18.677797Z","iopub.execute_input":"2022-01-08T20:02:18.678031Z","iopub.status.idle":"2022-01-08T20:02:18.683631Z","shell.execute_reply.started":"2022-01-08T20:02:18.677996Z","shell.execute_reply":"2022-01-08T20:02:18.682838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get unique images\nencode_train = sorted(set(all_img_name_vector))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)","metadata":{"id":"aANJdWTmOVqe","execution":{"iopub.status.busy":"2022-01-08T20:02:18.68656Z","iopub.execute_input":"2022-01-08T20:02:18.686768Z","iopub.status.idle":"2022-01-08T20:02:18.830006Z","shell.execute_reply.started":"2022-01-08T20:02:18.686745Z","shell.execute_reply":"2022-01-08T20:02:18.829324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())","metadata":{"id":"MTsxKfH2o0Ff","outputId":"809461df-60ac-421d-ce8e-d23b834ab773","execution":{"iopub.status.busy":"2022-01-08T20:02:18.831219Z","iopub.execute_input":"2022-01-08T20:02:18.831444Z","iopub.status.idle":"2022-01-08T20:04:40.800474Z","shell.execute_reply.started":"2022-01-08T20:02:18.831411Z","shell.execute_reply":"2022-01-08T20:04:40.799718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess and tokenize the captions","metadata":{"id":"iRFRPOnGo0JA","execution":{"iopub.status.busy":"2022-01-08T19:56:33.09281Z","iopub.status.idle":"2022-01-08T19:56:33.093222Z","shell.execute_reply.started":"2022-01-08T19:56:33.092989Z","shell.execute_reply":"2022-01-08T19:56:33.093012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the maximum length of any caption in the dataset\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)","metadata":{"id":"XwTg6KXPo0Mk","execution":{"iopub.status.busy":"2022-01-08T20:04:40.802228Z","iopub.execute_input":"2022-01-08T20:04:40.802423Z","iopub.status.idle":"2022-01-08T20:04:40.807004Z","shell.execute_reply.started":"2022-01-08T20:04:40.802397Z","shell.execute_reply":"2022-01-08T20:04:40.806295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\ntokenizer.fit_on_texts(train_captions2)","metadata":{"id":"TIc88JvTo0Qc","execution":{"iopub.status.busy":"2022-01-08T20:04:40.808452Z","iopub.execute_input":"2022-01-08T20:04:40.809257Z","iopub.status.idle":"2022-01-08T20:04:41.47023Z","shell.execute_reply.started":"2022-01-08T20:04:40.809222Z","shell.execute_reply":"2022-01-08T20:04:41.469475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","metadata":{"id":"mc4nHd7So0UR","execution":{"iopub.status.busy":"2022-01-08T20:04:41.471515Z","iopub.execute_input":"2022-01-08T20:04:41.471759Z","iopub.status.idle":"2022-01-08T20:04:41.47624Z","shell.execute_reply.started":"2022-01-08T20:04:41.471727Z","shell.execute_reply":"2022-01-08T20:04:41.475416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_captions2)","metadata":{"id":"pRsviwHNo0YL","execution":{"iopub.status.busy":"2022-01-08T20:04:49.833141Z","iopub.execute_input":"2022-01-08T20:04:49.833405Z","iopub.status.idle":"2022-01-08T20:04:50.349205Z","shell.execute_reply.started":"2022-01-08T20:04:49.833374Z","shell.execute_reply":"2022-01-08T20:04:50.348366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","metadata":{"id":"dRdTrp8fo0bS","execution":{"iopub.status.busy":"2022-01-08T20:04:51.266956Z","iopub.execute_input":"2022-01-08T20:04:51.267842Z","iopub.status.idle":"2022-01-08T20:04:51.574305Z","shell.execute_reply.started":"2022-01-08T20:04:51.267796Z","shell.execute_reply":"2022-01-08T20:04:51.573515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs)","metadata":{"id":"peq2hepoo0e5","execution":{"iopub.status.busy":"2022-01-08T20:04:52.638684Z","iopub.execute_input":"2022-01-08T20:04:52.63916Z","iopub.status.idle":"2022-01-08T20:04:52.645742Z","shell.execute_reply.started":"2022-01-08T20:04:52.639118Z","shell.execute_reply":"2022-01-08T20:04:52.644898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training, validation and testing","metadata":{"id":"2d4OszDFo0iK","execution":{"iopub.status.busy":"2022-01-08T20:04:53.522077Z","iopub.execute_input":"2022-01-08T20:04:53.52288Z","iopub.status.idle":"2022-01-08T20:04:53.527313Z","shell.execute_reply.started":"2022-01-08T20:04:53.522827Z","shell.execute_reply":"2022-01-08T20:04:53.526533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(refined, cap_vector):\n  img_to_cap_vector[img].append(cap)\n\n# Create training and validation sets using an 80-20 split randomly.\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_key = img_keys[:slice_index], img_keys[slice_index:]\n\nslice_index2 = int(len(img_name_val_key)*0.5)\nimg_name_test_keys, img_name_val_keys = img_name_val_key[:slice_index2], img_name_val_key[slice_index2:]\n\nimg_name_train = []\ncap_train = []\nfor imgtrain in img_name_train_keys:\n  cap_train_len = len(img_to_cap_vector[imgtrain])\n  img_name_train.extend([imgtrain] * cap_train_len)\n  cap_train.extend(img_to_cap_vector[imgtrain])\n\nimg_name_val = []\ncap_val = []\nfor imgv in img_name_val_keys:\n  capv_len = len(img_to_cap_vector[imgv])\n  img_name_val.extend([imgv] * capv_len)\n  cap_val.extend(img_to_cap_vector[imgv])\n\nimg_name_test = []\ncap_test = []\nfor imgtest in img_name_test_keys:\n  cap_test_len = len(img_to_cap_vector[imgtest])\n  img_name_test.extend([imgtest] * cap_test_len)\n  cap_test.append(img_to_cap_vector[imgtest])","metadata":{"id":"AApoJDESo0pi","execution":{"iopub.status.busy":"2022-01-08T20:46:10.32326Z","iopub.execute_input":"2022-01-08T20:46:10.323538Z","iopub.status.idle":"2022-01-08T20:46:10.384652Z","shell.execute_reply.started":"2022-01-08T20:46:10.323505Z","shell.execute_reply":"2022-01-08T20:46:10.383912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(img_name_train), len(cap_train), len(img_name_val), len(cap_val), len(img_name_test), len(cap_test)","metadata":{"id":"STsFg9gIo0sz","outputId":"c2a6051b-36a3-48e8-e94a-1d4e34d2ed55","execution":{"iopub.status.busy":"2022-01-08T20:04:55.292779Z","iopub.execute_input":"2022-01-08T20:04:55.293032Z","iopub.status.idle":"2022-01-08T20:04:55.298916Z","shell.execute_reply.started":"2022-01-08T20:04:55.293003Z","shell.execute_reply":"2022-01-08T20:04:55.298144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a tf.data dataset for training","metadata":{"id":"WXjxHiNyt7XG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feel free to change these parameters according to your system's configuration\n\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = top_k + 1\nnum_steps = len(img_name_train) // BATCH_SIZE\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\nfeatures_shape = 2048   # 1536\nattention_features_shape = 100 # EN b3 is 100","metadata":{"id":"Bb804WmUt7cl","execution":{"iopub.status.busy":"2022-01-08T20:04:59.8459Z","iopub.execute_input":"2022-01-08T20:04:59.846475Z","iopub.status.idle":"2022-01-08T20:04:59.851414Z","shell.execute_reply.started":"2022-01-08T20:04:59.846418Z","shell.execute_reply":"2022-01-08T20:04:59.850483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name, cap):\n  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n  return img_tensor, cap","metadata":{"id":"OWdnwalHt7hj","execution":{"iopub.status.busy":"2022-01-08T20:05:00.583163Z","iopub.execute_input":"2022-01-08T20:05:00.583515Z","iopub.status.idle":"2022-01-08T20:05:00.591386Z","shell.execute_reply.started":"2022-01-08T20:05:00.583444Z","shell.execute_reply":"2022-01-08T20:05:00.590737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"id":"eOgLs4LUt7m7","execution":{"iopub.status.busy":"2022-01-08T20:05:22.813118Z","iopub.execute_input":"2022-01-08T20:05:22.813388Z","iopub.status.idle":"2022-01-08T20:05:24.172173Z","shell.execute_reply.started":"2022-01-08T20:05:22.813354Z","shell.execute_reply":"2022-01-08T20:05:24.17148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model","metadata":{"id":"uA9y32Xct7rX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 64, units)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                         self.W2(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 64, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 64, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","metadata":{"id":"YOCOCNbst7w4","execution":{"iopub.status.busy":"2022-01-08T20:05:34.490803Z","iopub.execute_input":"2022-01-08T20:05:34.491078Z","iopub.status.idle":"2022-01-08T20:05:34.499675Z","shell.execute_reply.started":"2022-01-08T20:05:34.491044Z","shell.execute_reply":"2022-01-08T20:05:34.4982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","metadata":{"id":"5C4fgnsCt710","execution":{"iopub.status.busy":"2022-01-08T20:05:34.982927Z","iopub.execute_input":"2022-01-08T20:05:34.983824Z","iopub.status.idle":"2022-01-08T20:05:34.989746Z","shell.execute_reply.started":"2022-01-08T20:05:34.983772Z","shell.execute_reply":"2022-01-08T20:05:34.988924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))","metadata":{"id":"7IbQPRYBt77k","execution":{"iopub.status.busy":"2022-01-08T20:05:37.410557Z","iopub.execute_input":"2022-01-08T20:05:37.410818Z","iopub.status.idle":"2022-01-08T20:05:37.421325Z","shell.execute_reply.started":"2022-01-08T20:05:37.410787Z","shell.execute_reply":"2022-01-08T20:05:37.420451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)","metadata":{"id":"avkkoNzJt8Bb","execution":{"iopub.status.busy":"2022-01-08T20:05:40.231028Z","iopub.execute_input":"2022-01-08T20:05:40.231596Z","iopub.status.idle":"2022-01-08T20:05:40.255316Z","shell.execute_reply.started":"2022-01-08T20:05:40.231557Z","shell.execute_reply":"2022-01-08T20:05:40.254593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{"id":"tY0vn1Sst8G_","execution":{"iopub.status.busy":"2022-01-08T20:05:43.057695Z","iopub.execute_input":"2022-01-08T20:05:43.057978Z","iopub.status.idle":"2022-01-08T20:05:43.065262Z","shell.execute_reply.started":"2022-01-08T20:05:43.057944Z","shell.execute_reply":"2022-01-08T20:05:43.064225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checkpoint","metadata":{"id":"KO8qMsqht8Mb","execution":{"iopub.status.busy":"2022-01-08T20:05:44.35144Z","iopub.execute_input":"2022-01-08T20:05:44.352284Z","iopub.status.idle":"2022-01-08T20:05:44.356136Z","shell.execute_reply.started":"2022-01-08T20:05:44.352227Z","shell.execute_reply":"2022-01-08T20:05:44.355437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{"id":"QnLagOLuuhkl","execution":{"iopub.status.busy":"2022-01-08T20:05:46.133352Z","iopub.execute_input":"2022-01-08T20:05:46.133954Z","iopub.status.idle":"2022-01-08T20:05:46.144215Z","shell.execute_reply.started":"2022-01-08T20:05:46.133912Z","shell.execute_reply":"2022-01-08T20:05:46.14351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{"id":"_Dc3MLrWuhtV","execution":{"iopub.status.busy":"2022-01-08T20:05:48.40866Z","iopub.execute_input":"2022-01-08T20:05:48.409223Z","iopub.status.idle":"2022-01-08T20:05:48.413386Z","shell.execute_reply.started":"2022-01-08T20:05:48.409181Z","shell.execute_reply":"2022-01-08T20:05:48.412534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []","metadata":{"id":"Q93nBluZuhwy","execution":{"iopub.status.busy":"2022-01-08T20:05:48.699189Z","iopub.execute_input":"2022-01-08T20:05:48.699759Z","iopub.status.idle":"2022-01-08T20:05:48.703552Z","shell.execute_reply.started":"2022-01-08T20:05:48.699716Z","shell.execute_reply":"2022-01-08T20:05:48.702741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, trainable_variables)\n\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","metadata":{"id":"k6mTcRa3uh0i","execution":{"iopub.status.busy":"2022-01-08T20:05:51.060915Z","iopub.execute_input":"2022-01-08T20:05:51.061754Z","iopub.status.idle":"2022-01-08T20:05:51.070143Z","shell.execute_reply.started":"2022-01-08T20:05:51.061707Z","shell.execute_reply":"2022-01-08T20:05:51.069483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')","metadata":{"id":"i10xnU7huiU8","outputId":"bad83250-ac36-4a3e-a873-a4ee877dc701","execution":{"iopub.status.busy":"2022-01-08T20:06:00.538693Z","iopub.execute_input":"2022-01-08T20:06:00.538955Z","iopub.status.idle":"2022-01-08T20:20:32.979046Z","shell.execute_reply.started":"2022-01-08T20:06:00.538923Z","shell.execute_reply":"2022-01-08T20:20:32.977157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"id":"I-NfopCWuid9","outputId":"cbebf1f5-677d-42b4-e128-ce6fd42c9623","execution":{"iopub.status.busy":"2022-01-08T20:20:32.980922Z","iopub.execute_input":"2022-01-08T20:20:32.982082Z","iopub.status.idle":"2022-01-08T20:20:33.196223Z","shell.execute_reply.started":"2022-01-08T20:20:32.982045Z","shell.execute_reply":"2022-01-08T20:20:33.195527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caption!","metadata":{"id":"aRyGzQKpuinz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n                                                 -1,\n                                                 img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input,\n                                                         features,\n                                                         hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","metadata":{"id":"TjmtGgn0uixp","execution":{"iopub.status.busy":"2022-01-08T20:20:33.197575Z","iopub.execute_input":"2022-01-08T20:20:33.198Z","iopub.status.idle":"2022-01-08T20:20:33.207741Z","shell.execute_reply.started":"2022-01-08T20:20:33.197949Z","shell.execute_reply":"2022-01-08T20:20:33.206993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (10, 10)) # 10 * 10 = 100\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"lSvsW48AgJ43","execution":{"iopub.status.busy":"2022-01-08T20:20:33.209659Z","iopub.execute_input":"2022-01-08T20:20:33.209932Z","iopub.status.idle":"2022-01-08T20:20:33.220398Z","shell.execute_reply.started":"2022-01-08T20:20:33.209892Z","shell.execute_reply":"2022-01-08T20:20:33.219629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.save_weights('encoder.h5')\ndecoder.save_weights('decoder.h5')","metadata":{"id":"JkvYaXgVfj_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# captions on the validation set\nrid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nreal_caption = ' '.join([tokenizer.index_word[i]\n                        for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nprint('Real Caption:', real_caption)\nprint('Prediction Caption:', ' '.join(result))\nplot_attention(image, result, attention_plot)","metadata":{"id":"dj-8vbruKzc6","outputId":"ff2b0797-45dc-48c1-e155-bdd023c4b1dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing ","metadata":{"id":"bbB0onUuujJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pathy= '/content/Kindness_FTR.jpg'\n\nresult, attention_plot = evaluate(pathy)\nprint('Prediction Caption:', ' '.join(result))\n\nplot_attention(pathy, result, attention_plot)\n","metadata":{"id":"usd_mDJmujZb","outputId":"2e4e4448-2acc-4cfc-ff45-793b0d1b887f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image.open(pathy)","metadata":{"id":"BaXOrv2tgsLF","outputId":"7df38cf1-5dac-4fbe-b3f7-817fbac396e3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu","metadata":{"id":"9205I5OTSHmf","execution":{"iopub.status.busy":"2022-01-08T20:20:52.317692Z","iopub.execute_input":"2022-01-08T20:20:52.318178Z","iopub.status.idle":"2022-01-08T20:20:52.867424Z","shell.execute_reply.started":"2022-01-08T20:20:52.318138Z","shell.execute_reply":"2022-01-08T20:20:52.866665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run time ~ 30 minutes\ndef BLEU():\n  actual, predicted = list(), list()\n  \n  for i in range(len(img_name_test_keys)):\n    image = img_name_test_keys[i]\n    real_caption = []\n    for im in cap_test[i]:      \n        real_caption.append([tokenizer.index_word[w] for w in im if w n\\ot in [0]][1:-1])\n    # generate description\n    result, _ = evaluate(image)\n    # store actual and predicted\n\n    actual.append(real_caption)\n#     print(real_caption)\n    predicted.append(result[:-1])\n#     print(result[:-1])\n    print(i)\n  # calculate BLEU score\n  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))","metadata":{"id":"owFwuFs_ujhS","execution":{"iopub.status.busy":"2022-01-08T21:01:58.656527Z","iopub.execute_input":"2022-01-08T21:01:58.656797Z","iopub.status.idle":"2022-01-08T21:01:58.664956Z","shell.execute_reply.started":"2022-01-08T21:01:58.656766Z","shell.execute_reply":"2022-01-08T21:01:58.664277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BLEU()","metadata":{"id":"nlWb_Rl7X6Ih","outputId":"e56d8a40-cf5b-43be-ee02-dc8df057a93d","execution":{"iopub.status.busy":"2022-01-08T21:01:59.373615Z","iopub.execute_input":"2022-01-08T21:01:59.374079Z","iopub.status.idle":"2022-01-08T21:05:17.928932Z","shell.execute_reply.started":"2022-01-08T21:01:59.374039Z","shell.execute_reply":"2022-01-08T21:05:17.928114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"B06J-FShujpM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iFWX2xbXujxB"},"execution_count":null,"outputs":[]}]}